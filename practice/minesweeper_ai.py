# -*- coding: utf-8 -*-
import os
import numpy as np
import tensorflow as tf
from keras import layers, models, optimizers, mixed_precision
from keras.mixed_precision import global_policy, set_global_policy, Policy
import random
from collections import deque
import time
import tkinter as tk
from tkinter import messagebox
from minesweeper import MinesweeperGame, MainMenu
import itertools
import platform

# DirectML TensorFlow ì„¤ì • ë° ë©”ëª¨ë¦¬ ìµœì í™”
try:
	print("TensorFlow ë²„ì „:", tf.__version__)

	# DirectML ë””ë°”ì´ìŠ¤ ì„¤ì •
	# DirectMLì€ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
	# íŠ¹ë³„í•œ ì„¤ì •ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤

	# ì‚¬ìš© ê°€ëŠ¥í•œ DirectML ë””ë°”ì´ìŠ¤ í™•ì¸
	physical_devices = tf.config.list_physical_devices()
	print("ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¼ë¦¬ì  ë””ë°”ì´ìŠ¤:", physical_devices)

	# GPU ë””ë°”ì´ìŠ¤ í™•ì¸
	gpus = tf.config.list_physical_devices('GPU')
	if gpus:
		print(f"DirectML GPU ê°ì§€ë¨: {len(gpus)}ê°œ")
		for gpu in gpus:
			# GPU ë©”ëª¨ë¦¬ ì¦ê°€ ì„¤ì • (DirectMLì—ì„œ ì§€ì›í•˜ëŠ” ê²½ìš°)
			try:
				tf.config.experimental.set_memory_growth(gpu, True)
				print(f"ë©”ëª¨ë¦¬ ì¦ê°€ ì„¤ì • í™œì„±í™”: {gpu}")
			except:
				print(f"ë©”ëª¨ë¦¬ ì¦ê°€ ì„¤ì •ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ: {gpu}")

		# í˜¼í•© ì •ë°€ë„ ì„¤ì • (DirectMLì—ì„œ ì§€ì›í•˜ëŠ” ê²½ìš°)
		try:
			mixed_precision.set_global_policy('mixed_float16')
			print("í˜¼í•© ì •ë°€ë„ í™œì„±í™”: mixed_float16")
		except:
			print("í˜¼í•© ì •ë°€ë„ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ ì •ë°€ë„ ì‚¬ìš©")
	else:
		print("DirectML GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

	# CPU ì„¤ì • (Intel CPU ìµœì í™”)
	if platform.processor().startswith('Intel'):
		# Intel CPU ìµœì í™” ì„¤ì •
		os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'
		print("Intel CPU ìµœì í™” í™œì„±í™”")

	# DirectML ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥ (ë¬¸ì œ í•´ê²°ì— ë„ì›€ì´ ë¨)
	os.environ['TF_DIRECTML_VERBOSE'] = '1'

except Exception as e:
	print(f"DirectML ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
	print("ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ê³„ì†í•©ë‹ˆë‹¤.")


# ë©”íƒ€ í•™ìŠµì„ ìœ„í•œ í´ë˜ìŠ¤
class MetaLearningAgent:
	"""ì—¬ëŸ¬ ê²Œì„ êµ¬ì„±ì—ì„œ í•™ìŠµí•˜ê³  ì§€ì‹ì„ ì „ì´í•˜ëŠ” ë©”íƒ€ í•™ìŠµ ì—ì´ì „íŠ¸"""

	def __init__(self, configurations=None):
		"""
		Args:
			configurations: í•™ìŠµí•  ê²Œì„ êµ¬ì„± ëª©ë¡ [(width, height, mines), ...]
						   ê¸°ë³¸ê°’ì€ ì´ˆê¸‰, ì¤‘ê¸‰, ê³ ê¸‰ ë‚œì´ë„
		"""
		if configurations is None:
			# ê¸°ë³¸ êµ¬ì„±: ì´ˆê¸‰, ì¤‘ê¸‰, ê³ ê¸‰
			self.configurations = [
				(9, 9, 10),  # ì´ˆê¸‰
				(16, 16, 40),  # ì¤‘ê¸‰
				(30, 16, 99)  # ê³ ê¸‰
			]
		else:
			self.configurations = configurations

		# ê° êµ¬ì„±ë³„ AI ì—ì´ì „íŠ¸
		self.agents = {}

		# ê³µìœ  ê²½í—˜ ë©”ëª¨ë¦¬
		self.shared_memory = deque(maxlen=50000)

	def initialize_agents(self, use_inference=True, architecture='cnn'):
		"""ëª¨ë“  êµ¬ì„±ì— ëŒ€í•œ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
		for width, height, mines in self.configurations:
			config_key = f"{width}x{height}_{mines}"
			self.agents[config_key] = MinesweeperAI(
				width=width,
				height=height,
				mines=mines,
				use_inference=use_inference,
				architecture=architecture
			)

	def train_all(self, episodes_per_config=500, visualize=False, self_play=True):
		"""ëª¨ë“  êµ¬ì„±ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ"""
		for width, height, mines in self.configurations:
			config_key = f"{width}x{height}_{mines}"
			print(f"\ní•™ìŠµ ì‹œì‘: êµ¬ì„± {config_key} ({episodes_per_config} ì—í”¼ì†Œë“œ)")

			# í˜„ì¬ êµ¬ì„±ì— ëŒ€í•œ ì—ì´ì „íŠ¸ í•™ìŠµ
			self.agents[config_key].train(
				episodes=episodes_per_config,
				visualize=visualize,
				self_play=self_play
			)

			# í•™ìŠµëœ ê²½í—˜ ê³µìœ 
			self._share_experiences(config_key)

		# ê³µìœ ëœ ê²½í—˜ìœ¼ë¡œ ëª¨ë“  ì—ì´ì „íŠ¸ ì¶”ê°€ í•™ìŠµ
		self._train_from_shared_experiences()

	def _share_experiences(self, source_config_key):
		"""íŠ¹ì • êµ¬ì„±ì˜ ê²½í—˜ì„ ê³µìœ  ë©”ëª¨ë¦¬ì— ì¶”ê°€"""
		source_agent = self.agents[source_config_key]

		# ì—ì´ì „íŠ¸ì˜ ê²½í—˜ ë©”ëª¨ë¦¬ì—ì„œ ì¼ë¶€ ìƒ˜í”Œë§í•˜ì—¬ ê³µìœ  ë©”ëª¨ë¦¬ì— ì¶”ê°€
		if len(source_agent.agent.memory) > 0:
			# ìµœëŒ€ 1000ê°œ ê²½í—˜ ìƒ˜í”Œë§
			sample_size = min(1000, len(source_agent.agent.memory))
			experiences = random.sample(list(source_agent.agent.memory), sample_size)

			# ê³µìœ  ë©”ëª¨ë¦¬ì— ì¶”ê°€
			for experience in experiences:
				self.shared_memory.append(experience)

	def _train_from_shared_experiences(self, batch_size=64, epochs=5):
		"""ê³µìœ ëœ ê²½í—˜ìœ¼ë¡œ ëª¨ë“  ì—ì´ì „íŠ¸ ì¶”ê°€ í•™ìŠµ"""
		if len(self.shared_memory) < batch_size:
			print("ê³µìœ  ë©”ëª¨ë¦¬ì— ì¶©ë¶„í•œ ê²½í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
			return

		print(f"\nê³µìœ  ê²½í—˜ìœ¼ë¡œ ëª¨ë“  ì—ì´ì „íŠ¸ ì¶”ê°€ í•™ìŠµ (ê³µìœ  ë©”ëª¨ë¦¬ í¬ê¸°: {len(self.shared_memory)})")

		for config_key, agent in self.agents.items():
			print(f"ì—ì´ì „íŠ¸ {config_key} ì¶”ê°€ í•™ìŠµ ì¤‘...")

			# ê° ì—ì´ì „íŠ¸ì— ëŒ€í•´ ê³µìœ  ë©”ëª¨ë¦¬ì—ì„œ ë°°ì¹˜ í•™ìŠµ ìˆ˜í–‰
			for _ in range(epochs):
				# ë°°ì¹˜ ìƒ˜í”Œë§
				minibatch = random.sample(self.shared_memory, batch_size)

				# ë°°ì¹˜ í•™ìŠµ
				for state, action, reward, next_state, done in minibatch:
					# ìƒíƒœ í¬ê¸° ì¡°ì • (í•„ìš”í•œ ê²½ìš°)
					if state.shape[0] != agent.height or state.shape[1] != agent.width:
						# í¬ê¸°ê°€ ë‹¤ë¥¸ ê²½ìš° ìŠ¤í‚µ
						continue

					action_type, row, col = action

					# í–‰/ì—´ ë²”ìœ„ í™•ì¸
					if row >= agent.height or col >= agent.width:
						continue

					# ì—ì´ì „íŠ¸ í•™ìŠµ
					agent.agent.remember(state, action, reward, next_state, done)

				# ë¦¬í”Œë ˆì´ ìˆ˜í–‰
				agent.agent.replay()

	def play_game(self, width=9, height=9, mines=10):
		"""íŠ¹ì • êµ¬ì„±ìœ¼ë¡œ ê²Œì„ í”Œë ˆì´"""
		config_key = f"{width}x{height}_{mines}"

		if config_key in self.agents:
			self.agents[config_key].play_game()
		else:
			print(f"êµ¬ì„± {config_key}ì— ëŒ€í•œ ì—ì´ì „íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

			# ê°€ì¥ ê°€ê¹Œìš´ êµ¬ì„± ì°¾ê¸°
			closest_key = self._find_closest_config(width, height, mines)

			if closest_key:
				print(f"ê°€ì¥ ê°€ê¹Œìš´ êµ¬ì„± {closest_key}ì˜ ì—ì´ì „íŠ¸ë¡œ í”Œë ˆì´í•©ë‹ˆë‹¤.")
				self.agents[closest_key].play_game()
			else:
				print("í”Œë ˆì´í•  ìˆ˜ ìˆëŠ” ì—ì´ì „íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

	def _find_closest_config(self, width, height, mines):
		"""ì£¼ì–´ì§„ êµ¬ì„±ê³¼ ê°€ì¥ ê°€ê¹Œìš´ êµ¬ì„± ì°¾ê¸°"""
		if not self.agents:
			return None

		closest_key = None
		min_distance = float('inf')

		for config_key in self.agents:
			w, h, m = map(int, config_key.replace('x', '_').split('_'))

			# ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
			distance = ((w - width) ** 2 + (h - height) ** 2 + (m - mines) ** 2) ** 0.5

			if distance < min_distance:
				min_distance = distance
				closest_key = config_key

		return closest_key


# í™•ë¥ ì  ì¶”ë¡ ì„ ìœ„í•œ í´ë˜ìŠ¤
class ProbabilisticInference:
	"""ì§€ë¢°ì°¾ê¸° ê²Œì„ì—ì„œ í™•ë¥ ì  ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤"""

	def __init__(self, width, height):
		self.width = width
		self.height = height

	def analyze_board(self, state_matrix, board=None):
		"""ê²Œì„ ë³´ë“œë¥¼ ë¶„ì„í•˜ì—¬ ê° ì…€ì˜ ì§€ë¢° í™•ë¥  ê³„ì‚°"""
		# ê²°ê³¼ ì €ì¥ìš© í™•ë¥  ë§µ ì´ˆê¸°í™” (ê¸°ë³¸ê°’: -1 = ì•Œ ìˆ˜ ì—†ìŒ)
		probability_map = np.full((self.height, self.width), -1.0)

		# ì´ë¯¸ ì—´ë¦° ì…€ ë˜ëŠ” ê¹ƒë°œì´ ìˆëŠ” ì…€ì€ í™•ë¥  ì„¤ì •
		for row in range(self.height):
			for col in range(self.width):
				if state_matrix[row, col] > 0:  # ìˆ«ìê°€ ìˆëŠ” ì—´ë¦° ì…€
					probability_map[row, col] = 0.0  # ì§€ë¢° í™•ë¥  0%
				elif state_matrix[row, col] == BOARD_REPRESENTATION['flag']:
					probability_map[row, col] = 1.0  # ì§€ë¢° í™•ë¥  100%
				elif state_matrix[row, col] == BOARD_REPRESENTATION['empty']:
					probability_map[row, col] = 0.0  # ì§€ë¢° í™•ë¥  0%

		# ìˆ«ì ì…€ ì£¼ë³€ì˜ ë‹«íŒ ì…€ì— ëŒ€í•œ ì œì•½ ì¡°ê±´ ìˆ˜ì§‘
		constraints = []
		for row in range(self.height):
			for col in range(self.width):
				if state_matrix[row, col] > 0:  # ìˆ«ìê°€ ìˆëŠ” ì—´ë¦° ì…€
					# ì£¼ë³€ ë‹«íŒ ì…€ ì°¾ê¸°
					closed_cells = []
					for dr, dc in [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]:
						nr, nc = row + dr, col + dc
						if 0 <= nr < self.height and 0 <= nc < self.width:
							if state_matrix[nr, nc] == BOARD_REPRESENTATION['unknown']:
								closed_cells.append((nr, nc))

					# ì£¼ë³€ ê¹ƒë°œ ìˆ˜ ê³„ì‚°
					flag_count = 0
					for dr, dc in [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]:
						nr, nc = row + dr, col + dc
						if 0 <= nr < self.height and 0 <= nc < self.width:
							if state_matrix[nr, nc] == BOARD_REPRESENTATION['flag']:
								flag_count += 1

					# ì œì•½ ì¡°ê±´ ì¶”ê°€: ì£¼ë³€ ë‹«íŒ ì…€ ì¤‘ (ìˆ«ì - ê¹ƒë°œ ìˆ˜) ê°œê°€ ì§€ë¢°
					if closed_cells:
						constraints.append((closed_cells, int(state_matrix[row, col]) - flag_count))

		# ë‹¨ìˆœ ì œì•½ ì¡°ê±´ ì²˜ë¦¬
		self._apply_simple_constraints(probability_map, constraints)

		# ê³ ê¸‰ ì œì•½ ì¡°ê±´ ì²˜ë¦¬ (ì œì•½ ì¡°ê±´ ê°„ì˜ ê´€ê³„ ë¶„ì„)
		if len(constraints) > 1:
			self._apply_advanced_constraints(probability_map, constraints)

		# ì „ì—­ í™•ë¥  ê³„ì‚° (ë‚¨ì€ ì§€ë¢° ìˆ˜ ê¸°ë°˜)
		if board is not None:
			self._apply_global_probability(probability_map, state_matrix, board)

		return probability_map

	def _apply_simple_constraints(self, probability_map, constraints):
		"""ë‹¨ìˆœ ì œì•½ ì¡°ê±´ ì ìš© (í™•ì‹¤í•œ ì§€ë¢°/ì•ˆì „ ì…€ ì‹ë³„)"""
		for cells, mines in constraints:
			# ëª¨ë“  ë‹«íŒ ì…€ì´ ì§€ë¢°ì¸ ê²½ìš°
			if len(cells) == mines and mines > 0:
				for row, col in cells:
					probability_map[row, col] = 1.0

			# ì§€ë¢°ê°€ ì—†ëŠ” ê²½ìš° (ëª¨ë“  ì…€ì´ ì•ˆì „)
			elif mines == 0:
				for row, col in cells:
					probability_map[row, col] = 0.0

	def _apply_advanced_constraints(self, probability_map, constraints):
		"""ê³ ê¸‰ ì œì•½ ì¡°ê±´ ì ìš© (ì œì•½ ì¡°ê±´ ê°„ì˜ ê´€ê³„ ë¶„ì„)"""
		# ì œì•½ ì¡°ê±´ ê°„ì˜ ë¶€ë¶„ì§‘í•© ê´€ê³„ í™•ì¸
		for (cells1, mines1), (cells2, mines2) in itertools.combinations(constraints, 2):
			# cells1ì´ cells2ì˜ ë¶€ë¶„ì§‘í•©ì¸ ê²½ìš°
			if set(cells1).issubset(set(cells2)):
				# cells2 - cells1ì— ìˆëŠ” ì…€ë“¤ì˜ ì§€ë¢° ìˆ˜ëŠ” mines2 - mines1
				diff_cells = [cell for cell in cells2 if cell not in cells1]
				diff_mines = mines2 - mines1

				# ëª¨ë“  ì°¨ì´ ì…€ì´ ì§€ë¢°ì¸ ê²½ìš°
				if len(diff_cells) == diff_mines and diff_mines > 0:
					for row, col in diff_cells:
						probability_map[row, col] = 1.0

				# ì°¨ì´ ì…€ì— ì§€ë¢°ê°€ ì—†ëŠ” ê²½ìš°
				elif diff_mines == 0:
					for row, col in diff_cells:
						probability_map[row, col] = 0.0

			# cells2ê°€ cells1ì˜ ë¶€ë¶„ì§‘í•©ì¸ ê²½ìš°
			elif set(cells2).issubset(set(cells1)):
				# cells1 - cells2ì— ìˆëŠ” ì…€ë“¤ì˜ ì§€ë¢° ìˆ˜ëŠ” mines1 - mines2
				diff_cells = [cell for cell in cells1 if cell not in cells2]
				diff_mines = mines1 - mines2

				# ëª¨ë“  ì°¨ì´ ì…€ì´ ì§€ë¢°ì¸ ê²½ìš°
				if len(diff_cells) == diff_mines and diff_mines > 0:
					for row, col in diff_cells:
						probability_map[row, col] = 1.0

				# ì°¨ì´ ì…€ì— ì§€ë¢°ê°€ ì—†ëŠ” ê²½ìš°
				elif diff_mines == 0:
					for row, col in diff_cells:
						probability_map[row, col] = 0.0

	def _apply_global_probability(self, probability_map, state_matrix, board):
		"""ì „ì—­ í™•ë¥  ê³„ì‚° (ë‚¨ì€ ì§€ë¢° ìˆ˜ ê¸°ë°˜)"""
		# ë‚¨ì€ ì§€ë¢° ìˆ˜ ê³„ì‚°
		total_mines = sum(1 for row in board for cell in row if cell == 'X')
		flagged_mines = np.sum(state_matrix == BOARD_REPRESENTATION['flag'])
		remaining_mines = total_mines - flagged_mines

		# ì•„ì§ í™•ë¥ ì´ ê²°ì •ë˜ì§€ ì•Šì€ ì…€ ìˆ˜
		unknown_cells = np.sum(probability_map == -1.0)

		# ë‚¨ì€ ì…€ì´ ìˆê³  ë‚¨ì€ ì§€ë¢°ê°€ ìˆëŠ” ê²½ìš° ì „ì—­ í™•ë¥  ê³„ì‚°
		if unknown_cells > 0 and remaining_mines > 0:
			global_probability = remaining_mines / unknown_cells

			# ì•„ì§ í™•ë¥ ì´ ê²°ì •ë˜ì§€ ì•Šì€ ì…€ì— ì „ì—­ í™•ë¥  ì ìš©
			for row in range(self.height):
				for col in range(self.width):
					if probability_map[row, col] == -1.0:
						probability_map[row, col] = global_probability

	def get_safe_moves(self, state_matrix, board=None):
		"""ì•ˆì „í•œ ì´ë™(í™•ë¥  0%)ê³¼ ì§€ë¢°ê°€ í™•ì‹¤í•œ ìœ„ì¹˜(í™•ë¥  100%) ë°˜í™˜"""
		probability_map = self.analyze_board(state_matrix, board)

		safe_moves = []  # í™•ë¥  0%ì¸ ì…€ (ì•ˆì „í•œ ì´ë™)
		mine_cells = []  # í™•ë¥  100%ì¸ ì…€ (ì§€ë¢° í™•ì‹¤)

		for row in range(self.height):
			for col in range(self.width):
				if state_matrix[row, col] == BOARD_REPRESENTATION['unknown']:
					if probability_map[row, col] == 0.0:
						safe_moves.append((ACTION_LEFT_CLICK, row, col))
					elif probability_map[row, col] == 1.0:
						mine_cells.append((ACTION_RIGHT_CLICK, row, col))

		return safe_moves, mine_cells, probability_map


# ìë™ ì¬ì‹œì‘ì„ ìœ„í•œ MinesweeperGame í™•ì¥ í´ë˜ìŠ¤
class AutoRestartMinesweeperGame(MinesweeperGame):
	"""ì§€ë¢°ì°¾ê¸° ê²Œì„ í´ë˜ìŠ¤ í™•ì¥ - ìë™ ì¬ì‹œì‘ ê¸°ëŠ¥ ì¶”ê°€"""

	def __init__(self, master, width=10, height=10, mines=15, main_menu_callback=None):
		super().__init__(master, width, height, mines, main_menu_callback)
		# ë©”ì‹œì§€ ë°•ìŠ¤ ìë™ ì²˜ë¦¬ë¥¼ ìœ„í•œ í”Œë˜ê·¸
		self.auto_restart = True

	def handle_left_click(self, row, col):
		"""ì™¼ìª½ í´ë¦­ ì´ë²¤íŠ¸ ì²˜ë¦¬ - ë©”ì‹œì§€ ë°•ìŠ¤ ìë™ ì²˜ë¦¬"""
		# ê²Œì„ì´ ëë‚¬ìœ¼ë©´ ì•„ë¬´ ë™ì‘ ì•ˆí•¨
		if self.game_over:
			return

		# í”Œë˜ê·¸ê°€ ìˆëŠ” ì¹¸ì´ë©´ ì•„ë¬´ ë™ì‘ ì•ˆí•¨
		if self.buttons[row][col].cget('text') == 'ğŸš©':
			return

		# ì´ë¯¸ ì—´ë¦° ì…€ì¸ì§€ í™•ì¸
		if (row, col) in self.opened_cells:
			# ì´ë¯¸ ì—´ë¦° ì…€ì´ë©´ chord_click ì‹¤í–‰
			self.chord_click(row, col)
			return

		# ì²« í´ë¦­ ì‹œ íƒ€ì´ë¨¸ ì‹œì‘
		if self.start_time is None:
			self.start_time = time.time()

		# ì§€ë¢° í´ë¦­ ì‹œ ê²Œì„ ì˜¤ë²„
		if self.board[row][col] == 'X':
			self.buttons[row][col].config(text='ğŸ’£', bg='red')
			self.game_over = True
			self.reveal_all()

			if self.auto_restart:
				# ë©”ì‹œì§€ ë°•ìŠ¤ í‘œì‹œ ëŒ€ì‹  ìë™ìœ¼ë¡œ ê²Œì„ ì¬ì‹œì‘
				self.master.after(500, self.reset_game)
			else:
				messagebox.showinfo("ê²Œì„ ì˜¤ë²„", "ì§€ë¢°ë¥¼ ë°Ÿì•˜ìŠµë‹ˆë‹¤!")
			return

		# ë¹ˆ ì¹¸(0) í´ë¦­ ì‹œ ì£¼ë³€ ë¹ˆ ì¹¸ë“¤ ìë™ ì—´ê¸°
		self.reveal(row, col)

		# ìŠ¹ë¦¬ ì¡°ê±´ í™•ì¸
		self.check_win()

	def chord_click(self, row, col):
		"""ì½”ë“œ í´ë¦­ ì²˜ë¦¬ - ë©”ì‹œì§€ ë°•ìŠ¤ ìë™ ì²˜ë¦¬"""
		# ìˆ«ìê°€ ìˆëŠ” ì…€ë§Œ ì²˜ë¦¬
		cell_text = self.buttons[row][col].cget('text')
		if not cell_text or not cell_text.isdigit():
			return

		cell_number = int(cell_text)

		# ì£¼ë³€ ê¹ƒë°œ ìˆ˜ ê³„ì‚°
		flag_count = 0
		for i in range(max(0, row - 1), min(self.height, row + 2)):
			for j in range(max(0, col - 1), min(self.width, col + 2)):
				if self.buttons[i][j].cget('text') == 'ğŸš©':
					flag_count += 1

		# ì£¼ë³€ ì…€ ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸ (ê¹ƒë°œì´ ì•„ë‹Œ ë‹«íŒ ì…€)
		surrounding_cells = []
		for i in range(max(0, row - 1), min(self.height, row + 2)):
			for j in range(max(0, col - 1), min(self.width, col + 2)):
				if (i, j) != (row, col) and (i, j) not in self.opened_cells and self.buttons[i][j].cget('text') != 'ğŸš©':
					surrounding_cells.append((i, j))

		# í´ë¦­ íš¨ê³¼ ë³´ì—¬ì£¼ê¸°
		self.show_click_effect(surrounding_cells)

		# ê¹ƒë°œ ìˆ˜ê°€ ì…€ì˜ ìˆ«ìì™€ ì¼ì¹˜í•˜ë©´ ì£¼ë³€ ì…€ë“¤ ì—´ê¸°
		if flag_count == cell_number:
			for i, j in surrounding_cells:
				# ì§€ë¢° í´ë¦­ ì‹œ ê²Œì„ ì˜¤ë²„
				if self.board[i][j] == 'X':
					self.buttons[i][j].config(text='ğŸ’£', bg='red')
					self.game_over = True
					self.reveal_all()

					if self.auto_restart:
						# ë©”ì‹œì§€ ë°•ìŠ¤ í‘œì‹œ ëŒ€ì‹  ìë™ìœ¼ë¡œ ê²Œì„ ì¬ì‹œì‘
						self.master.after(500, self.reset_game)
					else:
						messagebox.showinfo("ê²Œì„ ì˜¤ë²„", "ì§€ë¢°ë¥¼ ë°Ÿì•˜ìŠµë‹ˆë‹¤!")
					return

				# ì…€ ì—´ê¸°
				self.reveal(i, j)

			# ìŠ¹ë¦¬ ì¡°ê±´ í™•ì¸
			self.check_win()

	def check_win(self):
		"""ìŠ¹ë¦¬ ì¡°ê±´ í™•ì¸ - ë©”ì‹œì§€ ë°•ìŠ¤ ìë™ ì²˜ë¦¬"""
		# ìŠ¹ë¦¬ ì¡°ê±´ 1: ëª¨ë“  ì§€ë¢°ê°€ ì•„ë‹Œ ì¹¸ì´ ì—´ë¦¼
		unopened = 0
		for i in range(self.height):
			for j in range(self.width):
				if self.buttons[i][j].cget('state') != 'disabled' and self.board[i][j] != 'X':
					unopened += 1

		# ìŠ¹ë¦¬ ì¡°ê±´ 2: ëª¨ë“  ì§€ë¢°ì— í”Œë˜ê·¸ê°€ ìˆìŒ
		correct_flags = 0
		for i in range(self.height):
			for j in range(self.width):
				if self.buttons[i][j].cget('text') == 'ğŸš©' and self.board[i][j] == 'X':
					correct_flags += 1

		if unopened == 0 or correct_flags == self.mines:
			self.game_over = True
			self.reveal_all()
			elapsed_time = int(time.time() - self.start_time)

			if self.auto_restart:
				# ë©”ì‹œì§€ ë°•ìŠ¤ í‘œì‹œ ëŒ€ì‹  ìë™ìœ¼ë¡œ ê²Œì„ ì¬ì‹œì‘
				self.master.after(500, self.reset_game)
			else:
				messagebox.showinfo("ìŠ¹ë¦¬", f"ì¶•í•˜í•©ë‹ˆë‹¤! ê²Œì„ì—ì„œ ì´ê²¼ìŠµë‹ˆë‹¤!\nì†Œìš” ì‹œê°„: {elapsed_time}ì´ˆ")


# Constants
BOARD_REPRESENTATION = {
	'unknown': 0,  # ì•„ì§ ì—´ë¦¬ì§€ ì•Šì€ ì…€
	'flag': -1,  # ê¹ƒë°œì´ ê½‚íŒ ì…€
	'mine': -2,  # ì§€ë¢° (í•™ìŠµìš©, ì‹¤ì œ ê²Œì„ì—ì„œëŠ” ë³´ì´ì§€ ì•ŠìŒ)
	'empty': 9,  # ë¹ˆ ì…€ (ìˆ«ìê°€ 0ì¸ ì…€)
	# 1-8: ì£¼ë³€ ì§€ë¢° ìˆ˜
}

# í–‰ë™ ì •ì˜
ACTION_LEFT_CLICK = 0  # ì™¼ìª½ í´ë¦­ (ì…€ ì—´ê¸°)
ACTION_RIGHT_CLICK = 1  # ì˜¤ë¥¸ìª½ í´ë¦­ (ê¹ƒë°œ ì„¤ì¹˜/ì œê±°)
ACTION_CHORD_CLICK = 2  # ì½”ë“œ í´ë¦­ (ì£¼ë³€ ì…€ ì—´ê¸°)


class MinesweeperState:
	"""ì§€ë¢°ì°¾ê¸° ê²Œì„ ìƒíƒœë¥¼ AIê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜"""

	def __init__(self, board, opened_cells, flags, width, height):
		self.width = width
		self.height = height
		self.board = board  # ì‹¤ì œ ê²Œì„ ë³´ë“œ (ì§€ë¢° ìœ„ì¹˜ í¬í•¨)
		self.opened_cells = opened_cells  # ì—´ë¦° ì…€ ì§‘í•©
		self.flags = flags  # ê¹ƒë°œì´ ì„¤ì¹˜ëœ ìœ„ì¹˜ ì§‘í•©

	def get_state_matrix(self, include_mines=False):
		"""í˜„ì¬ ê²Œì„ ìƒíƒœë¥¼ í–‰ë ¬ë¡œ ë³€í™˜"""
		state = np.zeros((self.height, self.width), dtype=np.int8)

		# ì—´ë¦° ì…€ í‘œì‹œ
		for row, col in self.opened_cells:
			if self.board[row][col] == '0':
				state[row][col] = BOARD_REPRESENTATION['empty']
			else:
				state[row][col] = int(self.board[row][col])

		# ê¹ƒë°œ í‘œì‹œ
		for row, col in self.flags:
			state[row][col] = BOARD_REPRESENTATION['flag']

		# í•™ìŠµì„ ìœ„í•´ ì§€ë¢° ìœ„ì¹˜ í¬í•¨ (ì‹¤ì œ ê²Œì„ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
		if include_mines:
			for row in range(self.height):
				for col in range(self.width):
					if self.board[row][col] == 'X' and (row, col) not in self.opened_cells and (row,
					                                                                            col) not in self.flags:
						state[row][col] = BOARD_REPRESENTATION['mine']

		return state

	def get_valid_actions(self):
		"""í˜„ì¬ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™ ëª©ë¡ ë°˜í™˜"""
		valid_actions = []

		for row in range(self.height):
			for col in range(self.width):
				# ì´ë¯¸ ì—´ë¦° ì…€ì€ ì½”ë“œ í´ë¦­ë§Œ ê°€ëŠ¥
				if (row, col) in self.opened_cells:
					# ìˆ«ìê°€ ìˆëŠ” ì…€ë§Œ ì½”ë“œ í´ë¦­ ê°€ëŠ¥
					if self.board[row][col] not in ['0', 'X']:
						valid_actions.append((ACTION_CHORD_CLICK, row, col))
				else:
					# ë‹«íŒ ì…€ì€ ì™¼ìª½/ì˜¤ë¥¸ìª½ í´ë¦­ ê°€ëŠ¥
					if (row, col) not in self.flags:
						valid_actions.append((ACTION_LEFT_CLICK, row, col))
					valid_actions.append((ACTION_RIGHT_CLICK, row, col))

		return valid_actions


class DQNAgent:
	"""Deep Q-Network ê¸°ë°˜ ì§€ë¢°ì°¾ê¸° AI ì—ì´ì „íŠ¸"""

	def __init__(self, width, height, memory_size=10000, batch_size=64, gamma=0.95, architecture='cnn'):
		self.width = width
		self.height = height
		self.state_shape = (height, width, 1)  # CNN ì…ë ¥ìš© í˜•íƒœ
		self.action_size = 3  # ì™¼ìª½ í´ë¦­, ì˜¤ë¥¸ìª½ í´ë¦­, ì½”ë“œ í´ë¦­

		# í•˜ì´í¼íŒŒë¼ë¯¸í„°
		self.memory = deque(maxlen=memory_size)
		self.batch_size = batch_size
		self.gamma = gamma  # í• ì¸ìœ¨
		self.epsilon = 1.0  # íƒí—˜ë¥ 
		self.epsilon_min = 0.01
		self.epsilon_decay = 0.995
		self.learning_rate = 0.001
		self.architecture = architecture  # ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì„ íƒ

		# ëª¨ë¸ ìƒì„±
		self.model = self._build_model()
		self.target_model = self._build_model()
		self.update_target_model()

	def _build_model(self):
		"""ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì— ë”°ë¥¸ Q-ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ ìƒì„±"""
		if self.architecture == 'cnn':
			return self._build_cnn_model()
		elif self.architecture == 'resnet':
			return self._build_resnet_model()
		elif self.architecture == 'densenet':
			return self._build_densenet_model()
		else:
			print(f"Unknown architecture: {self.architecture}, using CNN instead")
			return self._build_cnn_model()

	def _build_cnn_model(self):
		"""ê¸°ë³¸ CNN ì•„í‚¤í…ì²˜"""
		model = models.Sequential()

		# ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
		model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu',
		                        input_shape=self.state_shape))
		model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
		model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))

		# ì™„ì „ ì—°ê²° ë ˆì´ì–´
		model.add(layers.Flatten())
		model.add(layers.Dense(256, activation='relu'))
		model.add(layers.Dense(self.width * self.height * self.action_size))
		model.add(layers.Reshape((self.height, self.width, self.action_size)))

		model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=self.learning_rate))
		return model

	def _build_resnet_model(self):
		"""ResNet ì•„í‚¤í…ì²˜ (ì”ì°¨ ì—°ê²° í¬í•¨)"""
		inputs = layers.Input(shape=self.state_shape)

		# ì´ˆê¸° ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
		x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)

		# ì”ì°¨ ë¸”ë¡ 1
		residual = x
		x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)
		x = layers.Conv2D(64, (3, 3), padding='same')(x)
		x = layers.add([x, residual])
		x = layers.Activation('relu')(x)

		# ì”ì°¨ ë¸”ë¡ 2
		residual = x
		x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)
		x = layers.Conv2D(128, (3, 3), padding='same')(x)
		residual = layers.Conv2D(128, (1, 1), padding='same')(residual)  # ì°¨ì› ë§ì¶”ê¸°
		x = layers.add([x, residual])
		x = layers.Activation('relu')(x)

		# ì™„ì „ ì—°ê²° ë ˆì´ì–´
		x = layers.Flatten()(x)
		x = layers.Dense(256, activation='relu')(x)
		x = layers.Dense(self.width * self.height * self.action_size)(x)
		outputs = layers.Reshape((self.height, self.width, self.action_size))(x)

		model = models.Model(inputs=inputs, outputs=outputs)
		model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=self.learning_rate))
		return model

	def _build_densenet_model(self):
		"""DenseNet ì•„í‚¤í…ì²˜ (ë°€ì§‘ ì—°ê²° í¬í•¨)"""
		inputs = layers.Input(shape=self.state_shape)

		# ì´ˆê¸° ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
		x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)

		# ë°€ì§‘ ë¸”ë¡ 1
		dense1_outputs = [x]
		for _ in range(3):
			x = layers.Concatenate()(dense1_outputs)
			x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)
			dense1_outputs.append(x)

		# ì „í™˜ ë ˆì´ì–´
		x = layers.Concatenate()(dense1_outputs)
		x = layers.Conv2D(128, (1, 1), padding='same', activation='relu')(x)

		# ë°€ì§‘ ë¸”ë¡ 2
		dense2_outputs = [x]
		for _ in range(3):
			x = layers.Concatenate()(dense2_outputs)
			x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)
			dense2_outputs.append(x)

		# ì™„ì „ ì—°ê²° ë ˆì´ì–´
		x = layers.Concatenate()(dense2_outputs)
		x = layers.Flatten()(x)
		x = layers.Dense(256, activation='relu')(x)
		x = layers.Dense(self.width * self.height * self.action_size)(x)
		outputs = layers.Reshape((self.height, self.width, self.action_size))(x)

		model = models.Model(inputs=inputs, outputs=outputs)
		model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=self.learning_rate))
		return model

	def update_target_model(self):
		"""íƒ€ê²Ÿ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
		self.target_model.set_weights(self.model.get_weights())

	def remember(self, state, action, reward, next_state, done):
		"""ê²½í—˜ ì €ì¥"""
		self.memory.append((state, action, reward, next_state, done))

	def act(self, state, valid_actions):
		"""í˜„ì¬ ìƒíƒœì—ì„œ í–‰ë™ ì„ íƒ (GPU ê°€ì† ìµœì í™”)"""
		if np.random.rand() <= self.epsilon:
			# ë¬´ì‘ìœ„ í–‰ë™ ì„ íƒ
			return random.choice(valid_actions)

		# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í–‰ë™ ì„ íƒ
		state_tensor = np.expand_dims(state, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
		state_tensor = np.expand_dims(state_tensor, axis=-1)  # ì±„ë„ ì°¨ì› ì¶”ê°€

		# DirectML GPU ê°€ì† ì‚¬ìš© ì‹œë„
		try:
			# DirectMLì€ ìë™ìœ¼ë¡œ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤
			q_values = self.model.predict(state_tensor, verbose=0)[0]
		except Exception as e:
			print(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
			# ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ CPU ì—°ì‚°ìœ¼ë¡œ ì‹œë„
			with tf.device('/CPU:0'):
				q_values = self.model.predict(state_tensor, verbose=0)[0]

		# ìœ íš¨í•œ í–‰ë™ ì¤‘ì—ì„œ Qê°’ì´ ê°€ì¥ ë†’ì€ í–‰ë™ ì„ íƒ
		best_action = None
		best_q_value = float('-inf')

		for action_type, row, col in valid_actions:
			if q_values[row, col, action_type] > best_q_value:
				best_q_value = q_values[row, col, action_type]
				best_action = (action_type, row, col)

		return best_action

	def replay(self):
		"""ê²½í—˜ ë¦¬í”Œë ˆì´ë¥¼ í†µí•œ í•™ìŠµ (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)"""
		if len(self.memory) < self.batch_size:
			return

		# ë°°ì¹˜ í•™ìŠµì„ ìœ„í•œ ì¤€ë¹„
		minibatch = random.sample(self.memory, self.batch_size)

		# ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
		states = []
		targets = []

		# ë°°ì¹˜ ë‚´ ëª¨ë“  ë‹¤ìŒ ìƒíƒœì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•œ ë²ˆì— ìˆ˜í–‰
		next_states = []
		next_states_indices = []
		done_indices = []

		# ì²« ë²ˆì§¸ íŒ¨ìŠ¤: ìƒíƒœ ë° ë‹¤ìŒ ìƒíƒœ ë°ì´í„° ìˆ˜ì§‘
		for i, (state, action, reward, next_state, done) in enumerate(minibatch):
			# ìƒíƒœ í…ì„œ ì¤€ë¹„
			state_tensor = np.expand_dims(state, axis=-1)  # ì±„ë„ ì°¨ì› ì¶”ê°€
			states.append(state_tensor)

			# ì™„ë£Œë˜ì§€ ì•Šì€ ìƒíƒœì— ëŒ€í•´ ë‹¤ìŒ ìƒíƒœ ìˆ˜ì§‘
			if not done:
				next_state_tensor = np.expand_dims(next_state, axis=-1)
				next_states.append(next_state_tensor)
				next_states_indices.append(i)
			else:
				done_indices.append(i)

		# ë°°ì¹˜ë¡œ ë³€í™˜
		states = np.array(states)

		# ëª¨ë“  ìƒíƒœì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•œ ë²ˆì— ìˆ˜í–‰
		try:
			# DirectMLì€ ìë™ìœ¼ë¡œ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤
			predictions = self.model.predict(states, batch_size=self.batch_size, verbose=0)
		except Exception as e:
			print(f"ë°°ì¹˜ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
			# ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ CPU ì—°ì‚°ìœ¼ë¡œ ì‹œë„
			with tf.device('/CPU:0'):
				predictions = self.model.predict(states, batch_size=self.batch_size, verbose=0)

		# ë‹¤ìŒ ìƒíƒœê°€ ìˆëŠ” ê²½ìš° íƒ€ê²Ÿ ëª¨ë¸ë¡œ ì˜ˆì¸¡
		if next_states:
			next_states = np.array(next_states)
			try:
				# DirectMLì€ ìë™ìœ¼ë¡œ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤
				next_state_predictions = self.target_model.predict(next_states, batch_size=len(next_states), verbose=0)
			except Exception as e:
				print(f"ë‹¤ìŒ ìƒíƒœ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
				# ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ CPU ì—°ì‚°ìœ¼ë¡œ ì‹œë„
				with tf.device('/CPU:0'):
					next_state_predictions = self.target_model.predict(next_states, batch_size=len(next_states),
					                                                   verbose=0)

		# ë‘ ë²ˆì§¸ íŒ¨ìŠ¤: íƒ€ê²Ÿ ê°’ ê³„ì‚°
		for i, (state, action, reward, next_state, done) in enumerate(minibatch):
			action_type, row, col = action
			target = predictions[i].copy()

			if done:
				target[row, col, action_type] = reward
			else:
				# í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ë‹¤ìŒ ìƒíƒœ ì˜ˆì¸¡ ì°¾ê¸°
				next_state_idx = next_states_indices.index(i)
				next_state_pred = next_state_predictions[next_state_idx]
				target[row, col, action_type] = reward + self.gamma * np.amax(next_state_pred)

			targets.append(target)

		# ë°°ì¹˜ë¡œ ë³€í™˜
		targets = np.array(targets)

		# ë°°ì¹˜ í•™ìŠµ ìˆ˜í–‰ (í•œ ë²ˆì˜ fit í˜¸ì¶œë¡œ ëª¨ë“  ë°ì´í„° í•™ìŠµ)
		try:
			# DirectMLì€ ìë™ìœ¼ë¡œ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤
			self.model.fit(states, targets, batch_size=self.batch_size, epochs=1, verbose=0)
		except Exception as e:
			print(f"ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
			# ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ CPU ì—°ì‚°ìœ¼ë¡œ ì‹œë„
			with tf.device('/CPU:0'):
				self.model.fit(states, targets, batch_size=self.batch_size, epochs=1, verbose=0)

		# íƒí—˜ë¥  ê°ì†Œ
		if self.epsilon > self.epsilon_min:
			self.epsilon *= self.epsilon_decay

	def load(self, name):
		"""ëª¨ë¸ ë¡œë“œ"""
		if os.path.exists(name):
			self.model.load_weights(name)
			self.update_target_model()

	def save(self, name):
		"""ëª¨ë¸ ì €ì¥"""
		self.model.save_weights(name)


class MinesweeperAI:
	"""ì§€ë¢°ì°¾ê¸° ê²Œì„ì„ í”Œë ˆì´í•˜ëŠ” AI í´ë˜ìŠ¤"""

	def __init__(self, width=9, height=9, mines=10, use_inference=True, architecture='cnn'):
		self.width = width
		self.height = height
		self.mines = mines
		self.architecture = architecture
		self.agent = DQNAgent(width, height, architecture=architecture)
		self.model_path = f"minesweeper_model_{width}x{height}_{mines}_{architecture}.weights.h5"

		# í™•ë¥ ì  ì¶”ë¡  ì‚¬ìš© ì—¬ë¶€
		self.use_inference = use_inference
		if use_inference:
			self.inference = ProbabilisticInference(width, height)

		# ëª¨ë¸ ë¡œë“œ ì‹œë„
		self.agent.load(self.model_path)

	def train(self, episodes=1000, visualize=False, visualize_every=50, self_play=False):
		"""AI ì—ì´ì „íŠ¸ í•™ìŠµ"""
		# í•™ìŠµ ê²°ê³¼ ê¸°ë¡
		scores = []
		win_count = 0

		for episode in range(episodes):
			# ê²Œì„ í™˜ê²½ ì´ˆê¸°í™”
			if visualize and episode % visualize_every == 0:
				# ì‹œê°í™” ëª¨ë“œ
				self._train_episode_visualized(episode)
			elif self_play and episode % 5 == 0:  # 5 ì—í”¼ì†Œë“œë§ˆë‹¤ ìê°€ ëŒ€ì „
				# ìê°€ ëŒ€ì „ ëª¨ë“œ
				self._train_episode_self_play()
			else:
				# ë¹ ë¥¸ í•™ìŠµ ëª¨ë“œ
				score = self._train_episode_fast()
				scores.append(score)

				if score > 0:  # ìŠ¹ë¦¬
					win_count += 1

			# í•™ìŠµ ì§„í–‰ ìƒí™© ì¶œë ¥
			if (episode + 1) % 10 == 0:
				win_rate = win_count / 10 if episode >= 9 else win_count / (episode + 1)
				print(f"Episode: {episode + 1}/{episodes}, Win Rate: {win_rate:.2f}, Epsilon: {self.agent.epsilon:.4f}")
				win_count = 0

			# ì£¼ê¸°ì ìœ¼ë¡œ íƒ€ê²Ÿ ëª¨ë¸ ì—…ë°ì´íŠ¸ ë° ëª¨ë¸ ì €ì¥
			if (episode + 1) % 100 == 0:
				self.agent.update_target_model()
				self.agent.save(self.model_path)

		# ìµœì¢… ëª¨ë¸ ì €ì¥
		self.agent.save(self.model_path)
		return scores

	def _train_episode_self_play(self):
		"""ìê°€ ëŒ€ì „ì„ í†µí•œ í•™ìŠµ (CPU/GPU ìµœì í™”)"""
		# ê²Œì„ í™˜ê²½ ì´ˆê¸°í™”
		game = self._create_headless_game()

		# ê²Œì„ ìƒíƒœ ì´ˆê¸°í™”
		state = MinesweeperState(
			game.board,
			set(),
			set(),
			self.width,
			self.height
		)

		done = False
		moves = []  # ê²Œì„ ì¤‘ ìˆ˜í–‰í•œ ëª¨ë“  í–‰ë™ ê¸°ë¡
		states = []  # ê²Œì„ ì¤‘ ëª¨ë“  ìƒíƒœ ê¸°ë¡
		step_count = 0  # ìŠ¤í… ì¹´ìš´í„° ì¶”ê°€

		# ì²« ë²ˆì§¸ í–‰ë™ì€ í•­ìƒ ë¬´ì‘ìœ„ ìœ„ì¹˜ì— ì™¼ìª½ í´ë¦­ (ì²« í´ë¦­ì€ í•­ìƒ ì•ˆì „)
		row = random.randint(0, self.height - 1)
		col = random.randint(0, self.width - 1)

		# ì²« ë²ˆì§¸ í´ë¦­ ì‹¤í–‰
		self._perform_action(game, ACTION_LEFT_CLICK, row, col)

		# ê²Œì„ ìƒíƒœ ì—…ë°ì´íŠ¸
		state = MinesweeperState(
			game.board,
			game.opened_cells,
			{(r, c) for r in range(self.height) for c in range(self.width)
			 if game.buttons[r][c].cget('text') == 'ğŸš©'},
			self.width,
			self.height
		)

		# ê²Œì„ ë£¨í”„
		while not done:
			# í˜„ì¬ ìƒíƒœ í–‰ë ¬ ì–»ê¸°
			state_matrix = state.get_state_matrix()
			states.append(state_matrix.copy())

			# ìœ íš¨í•œ í–‰ë™ ëª©ë¡ ì–»ê¸°
			valid_actions = state.get_valid_actions()

			if not valid_actions:
				break

			# í•˜ì´ë¸Œë¦¬ë“œ í–‰ë™ ì„ íƒ (í™•ë¥ ì  ì¶”ë¡  + DQN)
			action = self.hybrid_act(state_matrix, valid_actions, game.board)
			action_type, row, col = action
			moves.append(action)
			step_count += 1

			# í–‰ë™ ì‹¤í–‰
			reward, done = self._perform_action(game, action_type, row, col)

			# ë‹¤ìŒ ìƒíƒœ ì–»ê¸°
			next_state = MinesweeperState(
				game.board,
				game.opened_cells,
				{(r, c) for r in range(self.height) for c in range(self.width)
				 if game.buttons[r][c].cget('text') == 'ğŸš©'},
				self.width,
				self.height
			)

			# ìƒíƒœ ì—…ë°ì´íŠ¸
			state = next_state

		# ê²Œì„ ê²°ê³¼ì— ë”°ë¥¸ ë³´ìƒ ê³„ì‚°
		final_reward = 10 if len(game.opened_cells) == (self.width * self.height - self.mines) else -10

		# ìê°€ ëŒ€ì „ í•™ìŠµ: ê²Œì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë“  í–‰ë™ í‰ê°€
		# ê²½í—˜ ë©”ëª¨ë¦¬ì— ì €ì¥ë§Œ í•˜ê³  í•™ìŠµì€ í•œ ë²ˆë§Œ ìˆ˜í–‰ (CPU ë¶€í•˜ ê°ì†Œ)
		for i in range(len(moves)):
			# í˜„ì¬ ìƒíƒœì™€ í–‰ë™
			state_matrix = states[i]
			action = moves[i]
			action_type, row, col = action

			# ë‹¤ìŒ ìƒíƒœ (ë§ˆì§€ë§‰ í–‰ë™ì¸ ê²½ìš° í˜„ì¬ ìƒíƒœ ì‚¬ìš©)
			next_state_matrix = states[i + 1] if i < len(moves) - 1 else state_matrix

			# ë³´ìƒ ê³„ì‚°: ë§ˆì§€ë§‰ í–‰ë™ì—ëŠ” ìµœì¢… ë³´ìƒ, ë‚˜ë¨¸ì§€ëŠ” ì‘ì€ ë³´ìƒ
			if i == len(moves) - 1:
				reward = final_reward
			else:
				reward = 0.1  # ì¤‘ê°„ í–‰ë™ì— ëŒ€í•œ ì‘ì€ ë³´ìƒ

			# ê²½í—˜ ì €ì¥
			self.agent.remember(state_matrix, action, reward, next_state_matrix, i == len(moves) - 1)

		# ë°°ì¹˜ í•™ìŠµ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
		try:
			# DirectMLì€ ìë™ìœ¼ë¡œ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤
			self.agent.replay()
		except Exception as e:
			print(f"ìê°€ ëŒ€ì „ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
			# ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ CPU ì—°ì‚°ìœ¼ë¡œ ì‹œë„
			with tf.device('/CPU:0'):
				self.agent.replay()

	def hybrid_act(self, state_matrix, valid_actions, board=None, exploration=True):
		"""í™•ë¥ ì  ì¶”ë¡ ê³¼ ë”¥ëŸ¬ë‹ì„ ê²°í•©í•œ í–‰ë™ ì„ íƒ"""
		if not self.use_inference:
			# í™•ë¥ ì  ì¶”ë¡ ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° DQN ì—ì´ì „íŠ¸ë§Œ ì‚¬ìš©
			return self.agent.act(state_matrix, valid_actions)

		# í™•ë¥ ì  ì¶”ë¡ ìœ¼ë¡œ ì•ˆì „í•œ ì´ë™ê³¼ ì§€ë¢° ìœ„ì¹˜ ì°¾ê¸°
		safe_moves, mine_cells, probability_map = self.inference.get_safe_moves(state_matrix, board)

		# ì•ˆì „í•œ ì´ë™ì´ ìˆìœ¼ë©´ ê·¸ ì¤‘ í•˜ë‚˜ ì„ íƒ
		if safe_moves:
			return random.choice(safe_moves)

		# ì§€ë¢°ê°€ í™•ì‹¤í•œ ìœ„ì¹˜ê°€ ìˆìœ¼ë©´ ê¹ƒë°œ ì„¤ì¹˜
		if mine_cells:
			return random.choice(mine_cells)

		# í™•ë¥ ì  ì¶”ë¡ ìœ¼ë¡œ ê²°ì •í•  ìˆ˜ ì—†ëŠ” ê²½ìš° DQN ì—ì´ì „íŠ¸ ì‚¬ìš©
		if exploration:
			return self.agent.act(state_matrix, valid_actions)
		else:
			# íƒí—˜ ì—†ì´ ìµœì ì˜ í–‰ë™ ì„ íƒ (ê²Œì„ í”Œë ˆì´ ëª¨ë“œ)
			original_epsilon = self.agent.epsilon
			self.agent.epsilon = 0
			action = self.agent.act(state_matrix, valid_actions)
			self.agent.epsilon = original_epsilon
			return action

	def _train_episode_fast(self):
		"""ì‹œê°í™” ì—†ì´ ë¹ ë¥´ê²Œ í•œ ì—í”¼ì†Œë“œ í•™ìŠµ (CPU/GPU ìµœì í™”)"""
		# ê²Œì„ í™˜ê²½ ì´ˆê¸°í™”
		game = self._create_headless_game()

		# ê²Œì„ ìƒíƒœ ì´ˆê¸°í™”
		state = MinesweeperState(
			game.board,
			set(),
			set(),
			self.width,
			self.height
		)

		done = False
		score = 0
		step_count = 0  # ìŠ¤í… ì¹´ìš´í„° ì¶”ê°€
		update_frequency = 4  # 4ìŠ¤í…ë§ˆë‹¤ í•™ìŠµ ìˆ˜í–‰ (CPU ë¶€í•˜ ê°ì†Œ)

		# ì²« ë²ˆì§¸ í–‰ë™ì€ í•­ìƒ ë¬´ì‘ìœ„ ìœ„ì¹˜ì— ì™¼ìª½ í´ë¦­ (ì²« í´ë¦­ì€ í•­ìƒ ì•ˆì „)
		row = random.randint(0, self.height - 1)
		col = random.randint(0, self.width - 1)

		# ì²« ë²ˆì§¸ í´ë¦­ ì‹¤í–‰
		self._perform_action(game, ACTION_LEFT_CLICK, row, col)

		# ê²Œì„ ìƒíƒœ ì—…ë°ì´íŠ¸
		state = MinesweeperState(
			game.board,
			game.opened_cells,
			{(r, c) for r in range(self.height) for c in range(self.width)
			 if game.buttons[r][c].cget('text') == 'ğŸš©'},
			self.width,
			self.height
		)

		# ê²Œì„ ë£¨í”„
		while not done:
			# í˜„ì¬ ìƒíƒœ í–‰ë ¬ ì–»ê¸°
			state_matrix = state.get_state_matrix()

			# ìœ íš¨í•œ í–‰ë™ ëª©ë¡ ì–»ê¸°
			valid_actions = state.get_valid_actions()

			if not valid_actions:
				break

			# í•˜ì´ë¸Œë¦¬ë“œ í–‰ë™ ì„ íƒ (í™•ë¥ ì  ì¶”ë¡  + DQN)
			action = self.hybrid_act(state_matrix, valid_actions, game.board)
			action_type, row, col = action

			# í–‰ë™ ì‹¤í–‰
			reward, done = self._perform_action(game, action_type, row, col)
			score += reward
			step_count += 1

			# ë‹¤ìŒ ìƒíƒœ ì–»ê¸°
			next_state = MinesweeperState(
				game.board,
				game.opened_cells,
				{(r, c) for r in range(self.height) for c in range(self.width)
				 if game.buttons[r][c].cget('text') == 'ğŸš©'},
				self.width,
				self.height
			)
			next_state_matrix = next_state.get_state_matrix()

			# ê²½í—˜ ì €ì¥
			self.agent.remember(state_matrix, action, reward, next_state_matrix, done)

			# ìƒíƒœ ì—…ë°ì´íŠ¸
			state = next_state

			# ì¼ì • ì£¼ê¸°ë§ˆë‹¤ ë˜ëŠ” ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ í•™ìŠµ ìˆ˜í–‰ (CPU ë¶€í•˜ ê°ì†Œ)
			if step_count % update_frequency == 0 or done:
				self.agent.replay()

		return score

	def _train_episode_visualized(self, episode):
		"""ì‹œê°í™”ì™€ í•¨ê»˜ í•œ ì—í”¼ì†Œë“œ í•™ìŠµ (CPU/GPU ìµœì í™”)"""
		# tkinter ë£¨íŠ¸ ìƒì„±
		root = tk.Tk()
		root.title(f"ì§€ë¢°ì°¾ê¸° AI í•™ìŠµ - ì—í”¼ì†Œë“œ {episode + 1}")

		# ê²Œì„ ìƒì„±
		game = MinesweeperGame(root, width=self.width, height=self.height, mines=self.mines)

		# ê²Œì„ ìƒíƒœ ì´ˆê¸°í™”
		state = MinesweeperState(
			game.board,
			game.opened_cells,
			{(r, c) for r in range(self.height) for c in range(self.width)
			 if game.buttons[r][c].cget('text') == 'ğŸš©'},
			self.width,
			self.height
		)

		done = False
		score = 0
		step_count = 0  # ìŠ¤í… ì¹´ìš´í„° ì¶”ê°€
		update_frequency = 4  # 4ìŠ¤í…ë§ˆë‹¤ í•™ìŠµ ìˆ˜í–‰ (CPU ë¶€í•˜ ê°ì†Œ)

		# ì²« ë²ˆì§¸ í–‰ë™ì€ í•­ìƒ ë¬´ì‘ìœ„ ìœ„ì¹˜ì— ì™¼ìª½ í´ë¦­ (ì²« í´ë¦­ì€ í•­ìƒ ì•ˆì „)
		row = random.randint(0, self.height - 1)
		col = random.randint(0, self.width - 1)

		# ì²« ë²ˆì§¸ í´ë¦­ ì‹¤í–‰ (ì‹œê°í™”)
		def first_click():
			self._perform_action_visualized(game, ACTION_LEFT_CLICK, row, col)
			root.after(100, game_loop)

		# ê²Œì„ ë£¨í”„ í•¨ìˆ˜
		def game_loop():
			nonlocal state, done, score, step_count

			# ê²Œì„ì´ ì¬ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸ (game_overê°€ Falseë¡œ ë³€ê²½ë¨)
			if hasattr(game, 'was_game_over') and game.was_game_over and not game.game_over:
				# ê²Œì„ì´ ì¬ì‹œì‘ë¨
				print("ê²Œì„ì´ ì¬ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
				game.was_game_over = False

				# ìƒíƒœ ì´ˆê¸°í™”
				state = MinesweeperState(
					game.board,
					game.opened_cells,
					{(r, c) for r in range(self.height) for c in range(self.width)
					 if game.buttons[r][c].cget('text') == 'ğŸš©'},
					self.width,
					self.height
				)

				# ì²« ë²ˆì§¸ í–‰ë™ì€ í•­ìƒ ë¬´ì‘ìœ„ ìœ„ì¹˜ì— ì™¼ìª½ í´ë¦­ (ì²« í´ë¦­ì€ í•­ìƒ ì•ˆì „)
				row = random.randint(0, self.height - 1)
				col = random.randint(0, self.width - 1)

				# ì²« ë²ˆì§¸ í´ë¦­ ì‹¤í–‰
				self._perform_action_visualized(game, ACTION_LEFT_CLICK, row, col)

				# ë‹¤ìŒ í–‰ë™ ì˜ˆì•½
				root.after(300, game_loop)
				return

			if done or game.game_over:
				# ê²Œì„ ì¢…ë£Œ ì²˜ë¦¬
				if game.game_over and not done:
					# ìŠ¹ë¦¬ ì—¬ë¶€ì— ë”°ë¥¸ ë³´ìƒ
					reward = 10 if len(game.opened_cells) == (self.width * self.height - self.mines) else -10
					score += reward
					done = True

					# ê²Œì„ ì˜¤ë²„ ìƒíƒœ ê¸°ë¡ (ì¬ì‹œì‘ ê°ì§€ìš©)
					game.was_game_over = True

					# í•™ìŠµ ìˆ˜í–‰ (ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ)
					self.agent.replay()

					# ì¼ì • ì‹œê°„ í›„ ë‹¤ì‹œ ê²Œì„ ë£¨í”„ ì‹¤í–‰ (ìë™ ì¬ì‹œì‘ í™•ì¸)
					root.after(1000, game_loop)
					return

				# ì—í”¼ì†Œë“œ ì¢…ë£Œ (ì°½ ë‹«ê¸°)
				root.after(1000, root.destroy)
				return

			# í˜„ì¬ ìƒíƒœ í–‰ë ¬ ì–»ê¸°
			state_matrix = state.get_state_matrix()

			# ìœ íš¨í•œ í–‰ë™ ëª©ë¡ ì–»ê¸°
			valid_actions = state.get_valid_actions()

			if not valid_actions:
				# í•™ìŠµ ìˆ˜í–‰ (ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ)
				self.agent.replay()
				root.after(1000, root.destroy)
				return

			# í•˜ì´ë¸Œë¦¬ë“œ í–‰ë™ ì„ íƒ (í™•ë¥ ì  ì¶”ë¡  + DQN)
			action = self.hybrid_act(state_matrix, valid_actions, game.board)
			action_type, row, col = action

			# í–‰ë™ ì‹¤í–‰ (ì‹œê°í™”)
			reward, done = self._perform_action_visualized(game, action_type, row, col)
			score += reward
			step_count += 1

			# ë‹¤ìŒ ìƒíƒœ ì–»ê¸°
			next_state = MinesweeperState(
				game.board,
				game.opened_cells,
				{(r, c) for r in range(self.height) for c in range(self.width)
				 if game.buttons[r][c].cget('text') == 'ğŸš©'},
				self.width,
				self.height
			)
			next_state_matrix = next_state.get_state_matrix()

			# ê²½í—˜ ì €ì¥
			self.agent.remember(state_matrix, action, reward, next_state_matrix, done)

			# ìƒíƒœ ì—…ë°ì´íŠ¸
			state = next_state

			# ì¼ì • ì£¼ê¸°ë§ˆë‹¤ í•™ìŠµ ìˆ˜í–‰ (CPU ë¶€í•˜ ê°ì†Œ)
			if step_count % update_frequency == 0:
				self.agent.replay()

			# ë‹¤ìŒ í–‰ë™ ì˜ˆì•½ (ì§€ì—° ì¶”ê°€)
			root.after(300, game_loop)

		# ì²« ë²ˆì§¸ í´ë¦­ ì˜ˆì•½
		root.after(500, first_click)

		# tkinter ë©”ì¸ ë£¨í”„ ì‹¤í–‰
		root.mainloop()

		return score

	def _create_headless_game(self):
		"""UI ì—†ì´ ê²Œì„ ë¡œì§ë§Œ ì‹¤í–‰í•˜ëŠ” ê²Œì„ ê°ì²´ ìƒì„±"""
		# ì„ì‹œ tkinter ë£¨íŠ¸ ìƒì„±
		root = tk.Tk()
		root.withdraw()  # UI ìˆ¨ê¸°ê¸°

		# ìë™ ì¬ì‹œì‘ ê²Œì„ ìƒì„±
		game = AutoRestartMinesweeperGame(root, width=self.width, height=self.height, mines=self.mines)

		return game

	def _perform_action(self, game, action_type, row, col):
		"""í–‰ë™ ì‹¤í–‰ ë° ë³´ìƒ ê³„ì‚° (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ)"""
		reward = 0
		done = False

		# í–‰ë™ ì‹¤í–‰ ì „ ì—´ë¦° ì…€ ìˆ˜
		opened_before = len(game.opened_cells)

		# í–‰ë™ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ë©”ì„œë“œ í˜¸ì¶œ
		if action_type == ACTION_LEFT_CLICK:
			game.handle_left_click(row, col)
		elif action_type == ACTION_RIGHT_CLICK:
			game.right_click(row, col)
		elif action_type == ACTION_CHORD_CLICK:
			game.chord_click(row, col)

		# í–‰ë™ ì‹¤í–‰ í›„ ì—´ë¦° ì…€ ìˆ˜
		opened_after = len(game.opened_cells)

		# ë³´ìƒ ê³„ì‚°
		if game.game_over:
			# ê²Œì„ ì¢…ë£Œ
			if len(game.opened_cells) == (self.width * self.height - self.mines):
				# ìŠ¹ë¦¬
				reward = 10
			else:
				# íŒ¨ë°° (ì§€ë¢° ë°ŸìŒ)
				reward = -10
			done = True
		else:
			# ìƒˆë¡œ ì—´ë¦° ì…€ ìˆ˜ì— ë¹„ë¡€í•˜ëŠ” ë³´ìƒ
			cells_opened = opened_after - opened_before
			reward = cells_opened * 0.1

			# ê¹ƒë°œ ê´€ë ¨ ë³´ìƒ
			if action_type == ACTION_RIGHT_CLICK:
				# ê¹ƒë°œì„ ì„¤ì¹˜í•œ ê²½ìš°
				if game.buttons[row][col].cget('text') == 'ğŸš©':
					# ì‹¤ì œ ì§€ë¢° ìœ„ì¹˜ì— ê¹ƒë°œì„ ì„¤ì¹˜í•˜ë©´ ì‘ì€ ë³´ìƒ
					if game.board[row][col] == 'X':
						reward = 0.2
					else:
						# ì§€ë¢°ê°€ ì•„ë‹Œ ê³³ì— ê¹ƒë°œì„ ì„¤ì¹˜í•˜ë©´ ì‘ì€ íŒ¨ë„í‹°
						reward = -0.1

		return reward, done

	def _perform_action_visualized(self, game, action_type, row, col):
		"""í–‰ë™ ì‹¤í–‰ ë° ë³´ìƒ ê³„ì‚° (ì‹œê°í™” ëª¨ë“œ)"""
		# í–‰ë™ ì‹¤í–‰ ì „ ì—´ë¦° ì…€ ìˆ˜
		opened_before = len(game.opened_cells)

		# í–‰ë™ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ë©”ì„œë“œ í˜¸ì¶œ
		if action_type == ACTION_LEFT_CLICK:
			# ì™¼ìª½ í´ë¦­ ì´ë²¤íŠ¸ ìƒì„±
			event = tk.Event()
			event.widget = game.buttons[row][col]
			game.handle_left_click(row, col)
		elif action_type == ACTION_RIGHT_CLICK:
			# ì˜¤ë¥¸ìª½ í´ë¦­ ì´ë²¤íŠ¸ ìƒì„±
			event = tk.Event()
			event.widget = game.buttons[row][col]
			game.right_click(row, col)
		elif action_type == ACTION_CHORD_CLICK:
			# ì½”ë“œ í´ë¦­ ì‹¤í–‰
			game.chord_click(row, col)

		# í–‰ë™ ì‹¤í–‰ í›„ ì—´ë¦° ì…€ ìˆ˜
		opened_after = len(game.opened_cells)

		# ë³´ìƒ ê³„ì‚°
		reward = 0
		done = False

		if game.game_over:
			# ê²Œì„ ì¢…ë£Œ
			if len(game.opened_cells) == (self.width * self.height - self.mines):
				# ìŠ¹ë¦¬
				reward = 10
			else:
				# íŒ¨ë°° (ì§€ë¢° ë°ŸìŒ)
				reward = -10
			done = True
		else:
			# ìƒˆë¡œ ì—´ë¦° ì…€ ìˆ˜ì— ë¹„ë¡€í•˜ëŠ” ë³´ìƒ
			cells_opened = opened_after - opened_before
			reward = cells_opened * 0.1

			# ê¹ƒë°œ ê´€ë ¨ ë³´ìƒ
			if action_type == ACTION_RIGHT_CLICK:
				# ê¹ƒë°œì„ ì„¤ì¹˜í•œ ê²½ìš°
				if game.buttons[row][col].cget('text') == 'ğŸš©':
					# ì‹¤ì œ ì§€ë¢° ìœ„ì¹˜ì— ê¹ƒë°œì„ ì„¤ì¹˜í•˜ë©´ ì‘ì€ ë³´ìƒ
					if game.board[row][col] == 'X':
						reward = 0.2
					else:
						# ì§€ë¢°ê°€ ì•„ë‹Œ ê³³ì— ê¹ƒë°œì„ ì„¤ì¹˜í•˜ë©´ ì‘ì€ íŒ¨ë„í‹°
						reward = -0.1

		return reward, done

	def play_game(self):
		"""í•™ìŠµëœ AIë¡œ ê²Œì„ í”Œë ˆì´ (ì‹œê°í™”)"""
		# tkinter ë£¨íŠ¸ ìƒì„±
		root = tk.Tk()
		root.title("ì§€ë¢°ì°¾ê¸° AI í”Œë ˆì´")

		# ê²Œì„ ìƒì„±
		game = MinesweeperGame(root, width=self.width, height=self.height, mines=self.mines)

		# ê²Œì„ ìƒíƒœ ì´ˆê¸°í™”
		state = MinesweeperState(
			game.board,
			game.opened_cells,
			{(r, c) for r in range(self.height) for c in range(self.width)
			 if game.buttons[r][c].cget('text') == 'ğŸš©'},
			self.width,
			self.height
		)

		# ì²« ë²ˆì§¸ í–‰ë™ì€ í•­ìƒ ë¬´ì‘ìœ„ ìœ„ì¹˜ì— ì™¼ìª½ í´ë¦­ (ì²« í´ë¦­ì€ í•­ìƒ ì•ˆì „)
		row = random.randint(0, self.height - 1)
		col = random.randint(0, self.width - 1)

		# ì²« ë²ˆì§¸ í´ë¦­ ì‹¤í–‰ (ì‹œê°í™”)
		def first_click():
			self._perform_action_visualized(game, ACTION_LEFT_CLICK, row, col)
			root.after(500, game_loop)

		# ê²Œì„ ë£¨í”„ í•¨ìˆ˜
		def game_loop():
			nonlocal state

			# ê²Œì„ì´ ì¬ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸ (game_overê°€ Falseë¡œ ë³€ê²½ë¨)
			if hasattr(game, 'was_game_over') and game.was_game_over and not game.game_over:
				# ê²Œì„ì´ ì¬ì‹œì‘ë¨
				print("ê²Œì„ì´ ì¬ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
				game.was_game_over = False

				# ìƒíƒœ ì´ˆê¸°í™”
				state = MinesweeperState(
					game.board,
					game.opened_cells,
					{(r, c) for r in range(self.height) for c in range(self.width)
					 if game.buttons[r][c].cget('text') == 'ğŸš©'},
					self.width,
					self.height
				)

				# ì²« ë²ˆì§¸ í–‰ë™ì€ í•­ìƒ ë¬´ì‘ìœ„ ìœ„ì¹˜ì— ì™¼ìª½ í´ë¦­ (ì²« í´ë¦­ì€ í•­ìƒ ì•ˆì „)
				row = random.randint(0, self.height - 1)
				col = random.randint(0, self.width - 1)

				# ì²« ë²ˆì§¸ í´ë¦­ ì‹¤í–‰
				self._perform_action_visualized(game, ACTION_LEFT_CLICK, row, col)

				# ë‹¤ìŒ í–‰ë™ ì˜ˆì•½
				root.after(500, game_loop)
				return

			if game.game_over:
				# ê²Œì„ ì¢…ë£Œ ë©”ì‹œì§€
				result = "ìŠ¹ë¦¬" if len(game.opened_cells) == (self.width * self.height - self.mines) else "íŒ¨ë°°"
				print(f"ê²Œì„ ì¢…ë£Œ: {result}")

				# ê²Œì„ ì˜¤ë²„ ìƒíƒœ ê¸°ë¡ (ì¬ì‹œì‘ ê°ì§€ìš©)
				game.was_game_over = True

				# ì¼ì • ì‹œê°„ í›„ ë‹¤ì‹œ ê²Œì„ ë£¨í”„ ì‹¤í–‰ (ìë™ ì¬ì‹œì‘ í™•ì¸)
				root.after(1000, game_loop)
				return

			# í˜„ì¬ ìƒíƒœ í–‰ë ¬ ì–»ê¸°
			state_matrix = state.get_state_matrix()

			# ìœ íš¨í•œ í–‰ë™ ëª©ë¡ ì–»ê¸°
			valid_actions = state.get_valid_actions()

			if not valid_actions:
				return

			# í•˜ì´ë¸Œë¦¬ë“œ í–‰ë™ ì„ íƒ (í™•ë¥ ì  ì¶”ë¡  + DQN, íƒí—˜ ì—†ì´)
			action = self.hybrid_act(state_matrix, valid_actions, game.board, exploration=False)
			action_type, row, col = action

			# í–‰ë™ ì‹¤í–‰ (ì‹œê°í™”)
			self._perform_action_visualized(game, action_type, row, col)

			# ë‹¤ìŒ ìƒíƒœ ì–»ê¸°
			next_state = MinesweeperState(
				game.board,
				game.opened_cells,
				{(r, c) for r in range(self.height) for c in range(self.width)
				 if game.buttons[r][c].cget('text') == 'ğŸš©'},
				self.width,
				self.height
			)

			# ìƒíƒœ ì—…ë°ì´íŠ¸
			state = next_state

			# ë‹¤ìŒ í–‰ë™ ì˜ˆì•½ (ì§€ì—° ì¶”ê°€)
			root.after(500, game_loop)

		# ì²« ë²ˆì§¸ í´ë¦­ ì˜ˆì•½
		root.after(1000, first_click)

		# tkinter ë©”ì¸ ë£¨í”„ ì‹¤í–‰
		root.mainloop()


# ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
	print("=" * 50)
	print("ì§€ë¢°ì°¾ê¸° AI - ë”¥ëŸ¬ë‹ ê¸°ë°˜ ìë™ í”Œë ˆì´")
	print("=" * 50)

	# ê³ ê¸‰ ê¸°ëŠ¥ ì‚¬ìš© ì—¬ë¶€
	use_advanced = input("\nê³ ê¸‰ ê¸°ëŠ¥ ì‚¬ìš©? (y/n, ê¸°ë³¸: n): ").lower() == 'y'

	if use_advanced:
		print("\nê³ ê¸‰ ê¸°ëŠ¥ ë©”ë‰´:")
		print("1. ë‹¨ì¼ ì—ì´ì „íŠ¸ (ê³ ê¸‰ ê¸°ëŠ¥ í¬í•¨)")
		print("2. ë©”íƒ€ í•™ìŠµ ì—ì´ì „íŠ¸ (ì—¬ëŸ¬ ë‚œì´ë„ í•™ìŠµ)")

		advanced_mode = input("\nì„ íƒ (ê¸°ë³¸: 1): ") or "1"

		if advanced_mode == "2":
			# ë©”íƒ€ í•™ìŠµ ì—ì´ì „íŠ¸ ì‚¬ìš©
			meta_agent = MetaLearningAgent()

			print("\nì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì„ íƒ:")
			print("1. CNN (ê¸°ë³¸)")
			print("2. ResNet")
			print("3. DenseNet")

			arch_choice = input("\nì„ íƒ (ê¸°ë³¸: 1): ") or "1"
			architecture = {
				"1": "cnn",
				"2": "resnet",
				"3": "densenet"
			}.get(arch_choice, "cnn")

			# í™•ë¥ ì  ì¶”ë¡  ì‚¬ìš© ì—¬ë¶€
			use_inference = input("\ní™•ë¥ ì  ì¶”ë¡  ì‚¬ìš©? (y/n, ê¸°ë³¸: y): ").lower() != 'n'

			# ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
			meta_agent.initialize_agents(use_inference=use_inference, architecture=architecture)

			print("\në©”íƒ€ í•™ìŠµ ëª¨ë“œ:")
			print("1. ëª¨ë“  ë‚œì´ë„ í•™ìŠµ")
			print("2. í•™ìŠµëœ ëª¨ë¸ë¡œ ê²Œì„ í”Œë ˆì´")

			meta_mode = input("\nì„ íƒ (ê¸°ë³¸: 1): ") or "1"

			if meta_mode == "1":
				# í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ ì„¤ì •
				episodes = int(input("\në‚œì´ë„ë³„ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸: 500): ") or "500")

				# ì‹œê°í™” ì—¬ë¶€ ì„¤ì •
				visualize = input("í•™ìŠµ ê³¼ì • ì‹œê°í™”? (y/n, ê¸°ë³¸: n): ").lower() == 'y'

				# ìê°€ ëŒ€ì „ ì‚¬ìš© ì—¬ë¶€
				self_play = input("ìê°€ ëŒ€ì „ ì‚¬ìš©? (y/n, ê¸°ë³¸: y): ").lower() != 'n'

				# í•™ìŠµ ì‹œì‘
				print("\në©”íƒ€ í•™ìŠµ ì‹œì‘")
				meta_agent.train_all(episodes_per_config=episodes, visualize=visualize, self_play=self_play)
				print("\në©”íƒ€ í•™ìŠµ ì™„ë£Œ")

				# í•™ìŠµ í›„ ê²Œì„ í”Œë ˆì´
				play_after_train = input("\ní•™ìŠµ í›„ ê²Œì„ í”Œë ˆì´? (y/n, ê¸°ë³¸: y): ").lower() != 'n'
				if play_after_train:
					# ë‚œì´ë„ ì„ íƒ
					print("\në‚œì´ë„ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
					print("1. ì´ˆê¸‰ (9x9, ì§€ë¢° 10ê°œ)")
					print("2. ì¤‘ê¸‰ (16x16, ì§€ë¢° 40ê°œ)")
					print("3. ê³ ê¸‰ (30x16, ì§€ë¢° 99ê°œ)")
					print("4. ì»¤ìŠ¤í…€ ì„¤ì •")

					difficulty = input("\nì„ íƒ (ê¸°ë³¸: 1): ") or "1"

					# ë‚œì´ë„ì— ë”°ë¥¸ ì„¤ì •
					if difficulty == "1":
						width, height, mines = 9, 9, 10
					elif difficulty == "2":
						width, height, mines = 16, 16, 40
					elif difficulty == "3":
						width, height, mines = 30, 16, 99
					elif difficulty == "4":
						# ì»¤ìŠ¤í…€ ì„¤ì •
						width = int(input("ê°€ë¡œ í¬ê¸° (8-30): ") or "9")
						height = int(input("ì„¸ë¡œ í¬ê¸° (8-24): ") or "9")
						max_mines = (width * height) - 9  # ìµœì†Œ 9ì¹¸ì€ ë¹„ì›Œë‘ 
						mines = int(input(f"ì§€ë¢° ìˆ˜ (1-{max_mines}): ") or "10")
					else:
						width, height, mines = 9, 9, 10

					meta_agent.play_game(width, height, mines)
			else:
				# ê²Œì„ í”Œë ˆì´
				# ë‚œì´ë„ ì„ íƒ
				print("\në‚œì´ë„ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
				print("1. ì´ˆê¸‰ (9x9, ì§€ë¢° 10ê°œ)")
				print("2. ì¤‘ê¸‰ (16x16, ì§€ë¢° 40ê°œ)")
				print("3. ê³ ê¸‰ (30x16, ì§€ë¢° 99ê°œ)")
				print("4. ì»¤ìŠ¤í…€ ì„¤ì •")

				difficulty = input("\nì„ íƒ (ê¸°ë³¸: 1): ") or "1"

				# ë‚œì´ë„ì— ë”°ë¥¸ ì„¤ì •
				if difficulty == "1":
					width, height, mines = 9, 9, 10
				elif difficulty == "2":
					width, height, mines = 16, 16, 40
				elif difficulty == "3":
					width, height, mines = 30, 16, 99
				elif difficulty == "4":
					# ì»¤ìŠ¤í…€ ì„¤ì •
					width = int(input("ê°€ë¡œ í¬ê¸° (8-30): ") or "9")
					height = int(input("ì„¸ë¡œ í¬ê¸° (8-24): ") or "9")
					max_mines = (width * height) - 9  # ìµœì†Œ 9ì¹¸ì€ ë¹„ì›Œë‘ 
					mines = int(input(f"ì§€ë¢° ìˆ˜ (1-{max_mines}): ") or "10")
				else:
					width, height, mines = 9, 9, 10

				meta_agent.play_game(width, height, mines)
		else:
			# ë‹¨ì¼ ì—ì´ì „íŠ¸ ê³ ê¸‰ ê¸°ëŠ¥
			# ë‚œì´ë„ ì„ íƒ
			print("\në‚œì´ë„ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
			print("1. ì´ˆê¸‰ (9x9, ì§€ë¢° 10ê°œ)")
			print("2. ì¤‘ê¸‰ (16x16, ì§€ë¢° 40ê°œ)")
			print("3. ê³ ê¸‰ (30x16, ì§€ë¢° 99ê°œ)")
			print("4. ì»¤ìŠ¤í…€ ì„¤ì •")

			difficulty = input("\nì„ íƒ (ê¸°ë³¸: 1): ") or "1"

			# ë‚œì´ë„ì— ë”°ë¥¸ ì„¤ì •
			if difficulty == "1":
				width, height, mines = 9, 9, 10
			elif difficulty == "2":
				width, height, mines = 16, 16, 40
			elif difficulty == "3":
				width, height, mines = 30, 16, 99
			elif difficulty == "4":
				# ì»¤ìŠ¤í…€ ì„¤ì •
				width = int(input("ê°€ë¡œ í¬ê¸° (8-30): ") or "9")
				height = int(input("ì„¸ë¡œ í¬ê¸° (8-24): ") or "9")
				max_mines = (width * height) - 9  # ìµœì†Œ 9ì¹¸ì€ ë¹„ì›Œë‘ 
				mines = int(input(f"ì§€ë¢° ìˆ˜ (1-{max_mines}): ") or "10")
			else:
				width, height, mines = 9, 9, 10

			print("\nì‹ ê²½ë§ ì•„í‚¤í…ì²˜ ì„ íƒ:")
			print("1. CNN (ê¸°ë³¸)")
			print("2. ResNet")
			print("3. DenseNet")

			arch_choice = input("\nì„ íƒ (ê¸°ë³¸: 1): ") or "1"
			architecture = {
				"1": "cnn",
				"2": "resnet",
				"3": "densenet"
			}.get(arch_choice, "cnn")

			# í™•ë¥ ì  ì¶”ë¡  ì‚¬ìš© ì—¬ë¶€
			use_inference = input("\ní™•ë¥ ì  ì¶”ë¡  ì‚¬ìš©? (y/n, ê¸°ë³¸: y): ").lower() != 'n'

			# ìê°€ ëŒ€ì „ ì‚¬ìš© ì—¬ë¶€
			self_play = input("\nìê°€ ëŒ€ì „ ì‚¬ìš©? (y/n, ê¸°ë³¸: y): ").lower() != 'n'

			# AI ì—ì´ì „íŠ¸ ìƒì„±
			ai = MinesweeperAI(width, height, mines, use_inference=use_inference, architecture=architecture)

			# í•™ìŠµ ëª¨ë“œ ë˜ëŠ” í”Œë ˆì´ ëª¨ë“œ ì„ íƒ
			mode = input("\nëª¨ë“œ ì„ íƒ (1: í•™ìŠµ, 2: í”Œë ˆì´): ")

			if mode == "1":
				# í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ ì„¤ì •
				episodes = int(input("\ní•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸: 1000): ") or "1000")

				# ì‹œê°í™” ì—¬ë¶€ ì„¤ì •
				visualize = input("í•™ìŠµ ê³¼ì • ì‹œê°í™”? (y/n, ê¸°ë³¸: n): ").lower() == 'y'

				# í•™ìŠµ ì‹œì‘
				print(f"\ní•™ìŠµ ì‹œì‘: {episodes} ì—í”¼ì†Œë“œ")
				ai.train(episodes=episodes, visualize=visualize, self_play=self_play)
				print("\ní•™ìŠµ ì™„ë£Œ")

				# í•™ìŠµ í›„ ê²Œì„ í”Œë ˆì´
				play_after_train = input("\ní•™ìŠµ í›„ ê²Œì„ í”Œë ˆì´? (y/n, ê¸°ë³¸: y): ").lower() != 'n'
				if play_after_train:
					ai.play_game()
			else:
				# í•™ìŠµëœ ëª¨ë¸ë¡œ ê²Œì„ í”Œë ˆì´
				ai.play_game()
	else:
		# ê¸°ë³¸ ê¸°ëŠ¥ (ì´ì „ ë²„ì „ê³¼ ë™ì¼)
		# ë‚œì´ë„ ì„¤ì •
		width, height, mines = 9, 9, 10  # ì´ˆê¸‰ ë‚œì´ë„

		# AI ì—ì´ì „íŠ¸ ìƒì„±
		ai = MinesweeperAI(width, height, mines)

		# í•™ìŠµ ëª¨ë“œ ë˜ëŠ” í”Œë ˆì´ ëª¨ë“œ ì„ íƒ
		mode = input("\nëª¨ë“œ ì„ íƒ (1: í•™ìŠµ, 2: í”Œë ˆì´): ")

		if mode == "1":
			# í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ ì„¤ì •
			episodes = int(input("\ní•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸: 1000): ") or "1000")

			# ì‹œê°í™” ì—¬ë¶€ ì„¤ì •
			visualize = input("í•™ìŠµ ê³¼ì • ì‹œê°í™”? (y/n, ê¸°ë³¸: n): ").lower() == 'y'

			# í•™ìŠµ ì‹œì‘
			print(f"\ní•™ìŠµ ì‹œì‘: {episodes} ì—í”¼ì†Œë“œ")
			ai.train(episodes=episodes, visualize=visualize)
			print("\ní•™ìŠµ ì™„ë£Œ")

			# í•™ìŠµ í›„ ê²Œì„ í”Œë ˆì´
			play_after_train = input("\ní•™ìŠµ í›„ ê²Œì„ í”Œë ˆì´? (y/n, ê¸°ë³¸: y): ").lower() != 'n'
			if play_after_train:
				ai.play_game()
		else:
			# í•™ìŠµëœ ëª¨ë¸ë¡œ ê²Œì„ í”Œë ˆì´
			ai.play_game()
